{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Cost Function\n",
    "Fill in the the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE / MAE\n",
    "    # ***************************************************\n",
    "    e = y-np.dot(tx,w)\n",
    "    n = e.shape[0]\n",
    "    return (1/(2*n))*np.dot(e,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search(y, tx, w0, w1):\n",
    "    \"\"\"Algorithm for grid search.\"\"\"\n",
    "    losses = np.zeros((len(w0), len(w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "import datetime\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e = y - np.dot(tx,w)\n",
    "    n = e.shape[0]\n",
    "    return -(1/n)*np.dot(tx.T,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma): \n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        g = compute_gradient(y,tx,w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma*g\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=2792.236712759167, w0=29.317568800842015, w1=5.391884973995655\n",
      "Gradient Descent(1/49): loss=1015.0521848293535, w0=46.90811008134726, w1=8.627015958393\n",
      "Gradient Descent(2/49): loss=375.2657547746187, w0=57.46243484965045, w1=10.5680945490314\n",
      "Gradient Descent(3/49): loss=144.94263995491332, w0=63.79502971063234, w1=11.732741703414455\n",
      "Gradient Descent(4/49): loss=62.02631861981966, w0=67.59458662722147, w1=12.431529996044297\n",
      "Gradient Descent(5/49): loss=32.176442939185925, w0=69.87432077717496, w1=12.85080297162219\n",
      "Gradient Descent(6/49): loss=21.430487694157765, w0=71.24216126714705, w1=13.10236675696893\n",
      "Gradient Descent(7/49): loss=17.561943805947603, w0=72.06286556113031, w1=13.253305028176976\n",
      "Gradient Descent(8/49): loss=16.169268006191956, w0=72.55528813752026, w1=13.343867990901805\n",
      "Gradient Descent(9/49): loss=15.667904718279921, w0=72.85074168335424, w1=13.398205768536702\n",
      "Gradient Descent(10/49): loss=15.487413934631583, w0=73.02801381085462, w1=13.43080843511764\n",
      "Gradient Descent(11/49): loss=15.422437252518186, w0=73.13437708735485, w1=13.450370035066204\n",
      "Gradient Descent(12/49): loss=15.399045646957367, w0=73.19819505325499, w1=13.462106995035342\n",
      "Gradient Descent(13/49): loss=15.390624668955466, w0=73.23648583279507, w1=13.469149171016824\n",
      "Gradient Descent(14/49): loss=15.387593116874784, w0=73.25946030051912, w1=13.473374476605713\n",
      "Gradient Descent(15/49): loss=15.386501758125739, w0=73.27324498115355, w1=13.475909659959047\n",
      "Gradient Descent(16/49): loss=15.386108868976084, w0=73.28151578953421, w1=13.477430769971047\n",
      "Gradient Descent(17/49): loss=15.385967428882205, w0=73.2864782745626, w1=13.478343435978248\n",
      "Gradient Descent(18/49): loss=15.385916510448414, w0=73.28945576557963, w1=13.478891035582569\n",
      "Gradient Descent(19/49): loss=15.385898179812248, w0=73.29124226018986, w1=13.47921959534516\n",
      "Gradient Descent(20/49): loss=15.385891580783225, w0=73.292314156956, w1=13.479416731202715\n",
      "Gradient Descent(21/49): loss=15.385889205132775, w0=73.29295729501567, w1=13.479535012717248\n",
      "Gradient Descent(22/49): loss=15.385888349898615, w0=73.29334317785148, w1=13.479605981625967\n",
      "Gradient Descent(23/49): loss=15.385888042014317, w0=73.29357470755296, w1=13.4796485629712\n",
      "Gradient Descent(24/49): loss=15.38588793117597, w0=73.29371362537385, w1=13.479674111778339\n",
      "Gradient Descent(25/49): loss=15.385887891274164, w0=73.29379697606639, w1=13.479689441062623\n",
      "Gradient Descent(26/49): loss=15.385887876909518, w0=73.29384698648191, w1=13.479698638633193\n",
      "Gradient Descent(27/49): loss=15.385887871738245, w0=73.29387699273123, w1=13.479704157175535\n",
      "Gradient Descent(28/49): loss=15.385887869876584, w0=73.29389499648082, w1=13.47970746830094\n",
      "Gradient Descent(29/49): loss=15.385887869206385, w0=73.29390579873056, w1=13.479709454976184\n",
      "Gradient Descent(30/49): loss=15.385887868965115, w0=73.29391228008042, w1=13.47971064698133\n",
      "Gradient Descent(31/49): loss=15.385887868878259, w0=73.29391616889033, w1=13.479711362184418\n",
      "Gradient Descent(32/49): loss=15.38588786884699, w0=73.29391850217627, w1=13.47971179130627\n",
      "Gradient Descent(33/49): loss=15.38588786883573, w0=73.29391990214783, w1=13.479712048779382\n",
      "Gradient Descent(34/49): loss=15.38588786883168, w0=73.29392074213078, w1=13.479712203263249\n",
      "Gradient Descent(35/49): loss=15.385887868830224, w0=73.29392124612055, w1=13.479712295953568\n",
      "Gradient Descent(36/49): loss=15.385887868829695, w0=73.2939215485144, w1=13.47971235156776\n",
      "Gradient Descent(37/49): loss=15.385887868829508, w0=73.29392172995071, w1=13.479712384936276\n",
      "Gradient Descent(38/49): loss=15.385887868829439, w0=73.29392183881251, w1=13.479712404957384\n",
      "Gradient Descent(39/49): loss=15.385887868829416, w0=73.29392190412958, w1=13.47971241697005\n",
      "Gradient Descent(40/49): loss=15.385887868829407, w0=73.29392194331982, w1=13.47971242417765\n",
      "Gradient Descent(41/49): loss=15.385887868829402, w0=73.29392196683396, w1=13.479712428502209\n",
      "Gradient Descent(42/49): loss=15.385887868829403, w0=73.29392198094246, w1=13.479712431096944\n",
      "Gradient Descent(43/49): loss=15.385887868829402, w0=73.29392198940755, w1=13.479712432653786\n",
      "Gradient Descent(44/49): loss=15.385887868829398, w0=73.29392199448661, w1=13.47971243358789\n",
      "Gradient Descent(45/49): loss=15.385887868829398, w0=73.29392199753404, w1=13.479712434148354\n",
      "Gradient Descent(46/49): loss=15.385887868829402, w0=73.2939219993625, w1=13.479712434484632\n",
      "Gradient Descent(47/49): loss=15.385887868829402, w0=73.29392200045957, w1=13.479712434686398\n",
      "Gradient Descent(48/49): loss=15.385887868829402, w0=73.29392200111782, w1=13.479712434807459\n",
      "Gradient Descent(49/49): loss=15.385887868829403, w0=73.29392200151277, w1=13.479712434880094\n",
      "Gradient Descent: execution time=0.011 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.4\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0.0, 0.0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-556982b50f30>\u001b[0m in \u001b[0;36mplot_figure\u001b[0;34m(n_iter)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     fig = gradient_descent_visualization(\n\u001b[0;32m----> 5\u001b[0;31m         gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_size_inches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grid_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "yn,xn=next(batch_iter(y,tx,1))\n",
    "print(yn.shape)\n",
    "print(xn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_stoch_gradient(y,tx, np.array([2,1])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient for batch data.\"\"\"\n",
    "    yn, xn = next(batch_iter(y,tx,1))\n",
    "    return -(yn-np.dot(xn,w))*xn.T\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_epochs, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        g = compute_stoch_gradient(y,tx,w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        w = w - gamma*g\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=2792.236712759167, w0=[ 35.94324426  35.94324426], w1=[ 41.66532684  41.66532684]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10000,) (10000,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5e7e297b35b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m gradient_losses, gradient_ws = stochastic_gradient_descent(\n\u001b[0;32m---> 12\u001b[0;31m     y, tx, w_initial, batch_size, max_iters, gamma)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-d9631fe41cd5>\u001b[0m in \u001b[0;36mstochastic_gradient_descent\u001b[0;34m(y, tx, initial_w, batch_size, max_epochs, gamma)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_stoch_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[1;31m# store w and loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e8ca331dcce2>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[1;31m# TODO: compute loss by MSE / MAE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[1;31m# ***************************************************\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10000,) (10000,2) "
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.4\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0.0, 0.0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-556982b50f30>\u001b[0m in \u001b[0;36mplot_figure\u001b[0;34m(n_iter)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     fig = gradient_descent_visualization(\n\u001b[0;32m----> 5\u001b[0;31m         gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_size_inches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grid_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "23c9cff2174241e7aba0c4a7bcad789f": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    },
    "f92086d1a6bd46888cef718d13066348": {
     "views": [
      {
       "cell_index": 19
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
